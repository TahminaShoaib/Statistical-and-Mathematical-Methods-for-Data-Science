trainDataIndex <- createDataPartition(diab$diabetes, p=0.7, list = F)
trainData <- diab[trainDataIndex, ]
testData <- diab[-trainDataIndex, ]
View(trainData)
table(trainData$diabetes)
# Down Sample
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "diabetes"],
y = trainData$diabetes)
table(down_train$diabetes)
# Up Sample (optional)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "diabetes"],
y = trainData$diabetes)
table(up_train$Class)
table(down_train$diabetes)
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "diabetes"],
y = trainData$diabetes)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "diabetes"],
y = trainData$diabetes)
table(up_train$diabetes)
View(down_train)
fulllogitmod <- glm(diabetes ~., family = "binomial", data=down_train)
coef(fulllogitmod)
fulllogitmod <- glm(diabetes ~., family = "binomial", data=trainData)
coef(fulllogitmod)
library(MASS)
steplogitmod <- stepAIC(fulllogitmod, trace = FALSE)
coef(steplogitmod)
pred <- predict(fulllogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$diabetes
mean(y_pred == y_act)
pred <- predict(steplogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
mean(y_pred == y_act)
pred <- predict(steplogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
y_pred
mean(y_pred == y_act)
# install.packages('mlbench')
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
correlationMatrix <- cor(ip[,1:34])
print(correlationMatrix)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
View(correlationMatrix)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
results <- rfe(ip[,1:34], ip[,35], sizes=c(1:34), rfeControl=control)
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
table(trainData$Class)
library(neuralnet)
nn <- neuralnet(Class ~ V5, V3, V27, V7, V8, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
library(neuralnet)
nn <- neuralnet(Class ~ V5+V3+V27+V7+V8, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
temp_test <- subset(testData, select = c("V5","V3", "V27", "V7", "V8"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
results
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]
bc <- bc[,-1]
# convert to numeric
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
# Change Y values to 1's and 0's
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
# Class distribution of train data
table(trainData$Class)
# Down Sample
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(down_train$Class)
# Up Sample (optional)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(up_train$Class)
logitmod <- glm(Class ~ Cl.thickness + Cell.size + Cell.shape, family = "binomial", data=down_train)
summary(logitmod)
pred <- predict(logitmod, newdata = testData, type = "response")
pred
# Recode factors
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
table(trainData$Class)
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(down_train$Class)
# Up Sample (optional)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(up_train$Class)
fulllogitmod <- glm(Class ~., family = "binomial", data=down_train)
coef(fulllogitmod)
library(MASS)
steplogitmod <- stepAIC(fulllogitmod, trace = FALSE)
coef(steplogitmod)
pred <- predict(fulllogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
# Prediction accuracy of the stepwise logistic regression model:
pred <- predict(steplogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]
# remove id column
bc <- bc[,-1]
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
scaleddata<-scale(bc)
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
library(neuralnet)
nn <- neuralnet(Class ~., data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("Cl.thickness","Cell.size", "Cell.shape"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
table(trainData$Class)
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ V5+V3+V27+V7+V8, data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("V5","V3", "V27", "V7", "V8"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
results
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
# Load data
# install.packages('mlbench')
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]  # keep complete rows
# remove id column
bc <- bc[,-1]
# convert to numeric
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
# Change Y values to 1's and 0's
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
# Class distribution of train data
table(trainData$Class)
set.seed(100)
down_train <- downSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(down_train$Class)
# Up Sample (optional)
set.seed(100)
up_train <- upSample(x = trainData[, colnames(trainData) %ni% "Class"],
y = trainData$Class)
table(up_train$Class)
logitmod <- glm(Class ~., family = "binomial", data=down_train)
library(MASS)
steplogitmod <- stepAIC(logitmod, trace = FALSE)
coef(steplogitmod)
summary(logitmod)
library(MASS)
steplogitmod <- stepAIC(logitmod, trace = FALSE)
coef(steplogitmod)
summary(logitmod)
pred <- predict(logitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
pred <- predict(steplogitmod, newdata = testData, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- testData$Class
# Accuracy
mean(y_pred == y_act)
data(BreastCancer, package="mlbench")
bc <- BreastCancer[complete.cases(BreastCancer), ]
# remove id column
bc <- bc[,-1]
for(i in 1:9) {
bc[, i] <- as.numeric(as.character(bc[, i]))
}
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(bc[,1:9], bc[,35], sizes=c(1:9), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(bc[,1:9], bc[,9], sizes=c(1:9), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
scaleddata<-scale(bc)
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
bc$Class <- factor(bc$Class, levels = c(0, 1))
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(bc$Class, p=0.7, list = F)
trainData <- bc[trainDataIndex, ]
testData <- bc[-trainDataIndex, ]
View(bc)
library(neuralnet)
nn <- neuralnet(Class ~ Mitoses,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
library(neuralnet)
nn <- neuralnet(Class ~ Mitoses,
data=trainData, hidden=c(1,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
library(neuralnet)
nn <- neuralnet(Class ~ Cl.thickness+Cell.size+Cell.shape,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
# install.packages('mlbench')
data(Ionosphere, package="mlbench")
ip <- Ionosphere[complete.cases(Ionosphere), ]
# convert to numeric
for(i in 1:9) {
ip[, i] <- as.numeric(as.character(ip[, i]))
}
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(ip[,1:34], ip[,35], sizes=c(1:34), rfeControl=control)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(ip[,1:34], ip[,35], sizes=c(1:34), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
ip$Class <- ifelse(ip$Class == "good", 1, 0)
ip$Class <- factor(ip$Class, levels = c(0, 1))
# Prep Training and Test data.
library(caret)
'%ni%' <- Negate('%in%')  # define 'not in' func
options(scipen=999)  # prevents printing scientific notations.
set.seed(100)
trainDataIndex <- createDataPartition(ip$Class, p=0.7, list = F)
trainData <- ip[trainDataIndex, ]
testData <- ip[-trainDataIndex, ]
# Class distribution of train data
table(trainData$Class)
library(neuralnet)
nn <- neuralnet(Class ~ V5+V3+V27+V7+V8,
data=trainData, hidden=c(2,1), linear.output=TRUE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testData, select = c("V5","V3", "V27", "V7", "V8"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = testData$Class, prediction = nn.results$net.result)
results
y_pred_num <- ifelse(results$prediction.1 > results$prediction.2, 0, 1)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_pred
y_act <- testData$Class
mean(y_pred == y_act)
table(y_act,y_pred)
X_train = as.matrix(read.table('train2_5.txt'))
Y_train = as.matrix(read.table('train2_5Labels.txt'))
X_test = as.matrix(read.table('test2_5.txt'))
Y_test = as.matrix(read.table('test2_5Labels.txt'))
# Find the regression coefficients
regressionCoefficients <- function(X, Y, lambda){
add_one_matrix <- cbind(new_col=1, X)
transpose <- t(add_one_matrix)
a <- nrow(transpose)
iden <- diag(a)
new_matrix <- transpose%*%add_one_matrix
mult <- lambda*iden
sum <- new_matrix + mult
inverse_matrix <- solve(sum)
mult_new <- transpose%*%Y
final <- inverse_matrix%*%mult_new
#returnValue(final)
}
# Finding the prediction values
predictions <- function(testX,regressionCoefficients){
add_one_matrix <- cbind(new_col=1, testX)
predicted_labels <- add_one_matrix%*%regressionCoefficients
}
# Finding the confusion matrix
Matrix <- function(prediction,actualLabels){
TP=FP=TN=FN=0
for(i in seq_len(nrow(actualLabels)))
if(prediction[i]==2 && actualLabels[i]==2){
TP <- TP + 1
}else if(prediction[i]==5 && actualLabels[i]==5){
TN <- TN + 1
}else if(prediction[i]==2 && actualLabels[i]==5){
FP <- FP + 1
}else if(prediction[i]==5 && actualLabels[i]==2){
FN <- FN + 1
}
CM <- list("TP" = TP, "FP" = FP, "TN" = TN, "FN" = FN)
return(CM)
}
setwd("E:/FAST/Semester-II/Statistical and Mathematical Methods for Data Science/Assignments/Assignment-4")
# Input data for regression
X_train = as.matrix(read.table('train2_5.txt'))
Y_train = as.matrix(read.table('train2_5Labels.txt'))
X_test = as.matrix(read.table('test2_5.txt'))
Y_test = as.matrix(read.table('test2_5Labels.txt'))
# Find the regression coefficients
regressionCoefficients <- function(X, Y, lambda){
add_one_matrix <- cbind(new_col=1, X)
transpose <- t(add_one_matrix)
a <- nrow(transpose)
iden <- diag(a)
new_matrix <- transpose%*%add_one_matrix
mult <- lambda*iden
sum <- new_matrix + mult
inverse_matrix <- solve(sum)
mult_new <- transpose%*%Y
final <- inverse_matrix%*%mult_new
#returnValue(final)
}
# Finding the prediction values
predictions <- function(testX,regressionCoefficients){
add_one_matrix <- cbind(new_col=1, testX)
predicted_labels <- add_one_matrix%*%regressionCoefficients
}
# Finding the confusion matrix
Matrix <- function(prediction,actualLabels){
TP=FP=TN=FN=0
for(i in seq_len(nrow(actualLabels)))
if(prediction[i]==2 && actualLabels[i]==2){
TP <- TP + 1
}else if(prediction[i]==5 && actualLabels[i]==5){
TN <- TN + 1
}else if(prediction[i]==2 && actualLabels[i]==5){
FP <- FP + 1
}else if(prediction[i]==5 && actualLabels[i]==2){
FN <- FN + 1
}
CM <- list("TP" = TP, "FP" = FP, "TN" = TN, "FN" = FN)
return(CM)
}
coefficient_values <- regressionCoefficients(X_test, Y_test, 0.001)
predicted_values <- predictions(X_test, coefficient_values)
predicted_values[1]
predicted_values[10]
predicted_values[15]
predicted_values[100]
predicted_values[120]
predicted_values[200]
coefficient_values <- regressionCoefficients(X_test, Y_test, 1)
predicted_values <- predictions(X_test, coefficient_values)
predicted_values[1]
predicted_values[10]
predicted_values[15]
predicted_values[100]
predicted_values[120]
predicted_values[200]
coefficient_values <- regressionCoefficients(X_test, Y_test, 1000)
predicted_values <- predictions(X_test, coefficient_values)
predicted_values[1]
predicted_values[10]
predicted_values[15]
predicted_values[100]
predicted_values[120]
predicted_values[200]
coefficient_values <- regressionCoefficients(X_test, Y_test, 0.2)
predicted_values <- predictions(X_test, coefficient_values)
predicted_values[1]
predicted_values[10]
predicted_values[15]
predicted_values[100]
predicted_values[120]
predicted_values[200]
